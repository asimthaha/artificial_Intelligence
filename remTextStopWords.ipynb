{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2g4kSj77rdktjyG6V1pJw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os,sys\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/Datasets\")\n","sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks/Datasets\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVHCDBoumLWc","executionInfo":{"status":"ok","timestamp":1692160287671,"user_tz":-330,"elapsed":4461,"user":{"displayName":"22PMC118 Asim Thaha Azeez","userId":"12011618729708686280"}},"outputId":"18c85c1f-9f0d-456a-a2ea-1ab186818051"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6O3ulDzUlpSQ","executionInfo":{"status":"ok","timestamp":1692160287672,"user_tz":-330,"elapsed":6,"user":{"displayName":"22PMC118 Asim Thaha Azeez","userId":"12011618729708686280"}}},"outputs":[],"source":["# Import libraries\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import spacy\n","\n","# Download NLTK resources (uncomment the following lines if you haven't downloaded them before)\n","# nltk.download('punkt')\n","# nltk.download('stopwords')\n","# nltk.download('wordnet')"]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbRbvgfWnWDS","executionInfo":{"status":"ok","timestamp":1692160324177,"user_tz":-330,"elapsed":1214,"user":{"displayName":"22PMC118 Asim Thaha Azeez","userId":"12011618729708686280"}},"outputId":"e89cba2d-7a83-4872-93db-be78a550913a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Load spaCy English model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Read data from text file\n","with open('intro.txt', 'r') as file:\n","    data = file.read()\n","\n","# Tokenize sentences using NLTK\n","sentences = sent_tokenize(data)\n","\n","# Initialize NLTK lemmatizer and spaCy\n","lemmatizer = WordNetLemmatizer()"],"metadata":{"id":"KZ0uFqE9lyuU","executionInfo":{"status":"ok","timestamp":1692160328143,"user_tz":-330,"elapsed":1757,"user":{"displayName":"22PMC118 Asim Thaha Azeez","userId":"12011618729708686280"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["nltk.download('wordnet')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pJdz0ekndq6","executionInfo":{"status":"ok","timestamp":1692160387839,"user_tz":-330,"elapsed":628,"user":{"displayName":"22PMC118 Asim Thaha Azeez","userId":"12011618729708686280"}},"outputId":"3a9a6788-e2af-433f-a989-cf3d89b2c8c6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Process sentences\n","processed_sentences = []\n","for sentence in sentences:\n","    # Tokenize words using NLTK\n","    words = word_tokenize(sentence)\n","\n","    # Lemmatize words using NLTK\n","    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n","\n","    # Remove stopwords using NLTK\n","    filtered_words = [word for word in lemmatized_words if word.lower() not in stopwords.words('english')]\n","\n","    # Join words to form the sentence\n","    processed_sentence = ' '.join(filtered_words)\n","\n","    processed_sentences.append(processed_sentence)"],"metadata":{"id":"SxE0wcVAl37S","executionInfo":{"status":"ok","timestamp":1692160391308,"user_tz":-330,"elapsed":691,"user":{"displayName":"22PMC118 Asim Thaha Azeez","userId":"12011618729708686280"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Write processed sentences to a new text file\n","with open('output.txt', 'w') as file:\n","    for sentence in processed_sentences:\n","        file.write(\"%s\\n\" % sentence)"],"metadata":{"id":"8qE-csifl6n5","executionInfo":{"status":"ok","timestamp":1692160404383,"user_tz":-330,"elapsed":413,"user":{"displayName":"22PMC118 Asim Thaha Azeez","userId":"12011618729708686280"}}},"execution_count":14,"outputs":[]}]}